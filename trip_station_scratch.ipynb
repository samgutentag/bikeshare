{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import datetime\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trip Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Trip Data...\n",
      "\tFinished file! (1 of 4)\n",
      "\tFinished file! (2 of 4)\n",
      "\tFinished file! (3 of 4)\n",
      "\tFinished file! (4 of 4)\n",
      "Data Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "print('Loading Trip Data...')\n",
    "\n",
    "try:\n",
    "    file_path_slug = '../../datasets/bayareabikeshare/*_trip_data.csv'\n",
    "\n",
    "    # glob all files\n",
    "    file_list = glob(file_path_slug)\n",
    "\n",
    "    trip_import = pd.DataFrame()\n",
    "\n",
    "    counter = 1\n",
    "    chunks = []\n",
    "\n",
    "    # load data from each file\n",
    "    for file in file_list:\n",
    "\n",
    "        # import file in chunks\n",
    "        for chunk in pd.read_csv(file, chunksize=10000, iterator=True):\n",
    "\n",
    "            # set chunk index column to 'Trip ID'\n",
    "            chunk = chunk.set_index('Trip ID')\n",
    "\n",
    "            # define Columns\n",
    "            chunk.columns = ['Duration', 'Start Date', 'Start Station', 'Start Terminal', 'End Date', \n",
    "                             'End Station', 'End Terminal', 'Bike #', 'Subscriber Type', 'Zip Code']\n",
    "\n",
    "            # append chunk to chunks list\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        print('\\tFinished file! (%d of %d)' % (counter, len(file_list)))\n",
    "        counter += 1\n",
    "\n",
    "    # concat chunks\n",
    "    trip_import = pd.concat(chunks)\n",
    "\n",
    "    print('Data Loaded Successfully!')\n",
    "\n",
    "except:\n",
    "    print('oops... something went wrong importing the data :(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zipcodes are all over the place, only keep corrected 5 digit zipcodes, and replace all others with NaNs\n",
    "def clean_zipcode(item):\n",
    "    if len(item) != 5:\n",
    "\n",
    "        # split on '-'\n",
    "        try:\n",
    "            result = item.split('-')[0]\n",
    "        except:\n",
    "            result = item\n",
    "\n",
    "        # split on '.'\n",
    "        try:\n",
    "            result = item.split('.')[0]\n",
    "        except:\n",
    "            result = item\n",
    "        \n",
    "        # if len of item is less than 5, return 'NaN'\n",
    "        if len(result) < 5:\n",
    "            result = 'NaN'\n",
    "        else:\n",
    "            # if len result is greater than 5, take at most, first 5 digits\n",
    "            result = result[:5]\n",
    "    else:\n",
    "        result = item\n",
    "    \n",
    "    # make sure result is all digits\n",
    "    if result.isdigit():\n",
    "        return result\n",
    "    else:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip Data Cleanup Started...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trip_import' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b16b3c0c9bc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trip Data Cleanup Started...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrip_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrip_import\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# cleanup column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\tcleaning column names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trip_import' is not defined"
     ]
    }
   ],
   "source": [
    "print('Trip Data Cleanup Started...')\n",
    "trip_data = trip_import.copy()\n",
    "\n",
    "# cleanup column names\n",
    "print('\\tcleaning column names')\n",
    "new_cols = []\n",
    "for col in trip_data.columns:\n",
    "    new_cols.append(col.replace(' ', '_').lower())\n",
    "trip_data.columns = new_cols\n",
    "\n",
    "# extract columns we want to keep\n",
    "print('\\tsubsetting to useful columns')\n",
    "important_cols = ['duration', 'start_date', 'start_terminal', 'end_date', 'end_terminal', 'bike_#', 'subscriber_type', 'zip_code']\n",
    "trip_data = trip_data[important_cols]\n",
    "\n",
    "# we are only looking at stations in San Francisco\n",
    "# sf_trips_data = trip_data[trip_data['start_terminal'].isin(sf_stations)]\n",
    "# sf_trips_data = sf_trips_data[sf_trips_data['end_terminal'].isin(sf_stations)]\n",
    "\n",
    "# trip_data = sf_trips_data.copy()\n",
    "\n",
    "# create duration minutes column\n",
    "print('\\tcreating a duration_minutes column')\n",
    "trip_data['duration_minutes'] = trip_data['duration'] / 60.0\n",
    "\n",
    "# convert end and start dates to datetime objects\n",
    "print('\\tconverting end and start dates to datetime objects')\n",
    "trip_data['start_date'] = pd.to_datetime(trip_data['start_date'], format=\"%m/%d/%Y %H:%M\")\n",
    "trip_data['end_date']   = pd.to_datetime(trip_data['end_date'],   format=\"%m/%d/%Y %H:%M\")\n",
    "\n",
    "# convert and clean zipcodes\n",
    "print('\\tcleaning zipcodes')\n",
    "trip_data['zip_code'] = trip_data['zip_code'].astype(str)\n",
    "trip_data.zip_code = trip_data.zip_code.apply(clean_zipcode)\n",
    "trip_data['zip_code'] = pd.to_numeric(trip_data['zip_code'], errors='coerce')\n",
    "\n",
    "# clean up data types\n",
    "print('cleaning up data types')\n",
    "\n",
    "trip_data['duration']         = trip_data['duration'].astype('float')\n",
    "trip_data['start_terminal']   = trip_data['start_terminal'].astype('category')\n",
    "trip_data['end_terminal']     = trip_data['end_terminal'].astype('category')\n",
    "trip_data['bike_#']           = trip_data['bike_#'].astype('int')\n",
    "trip_data['subscriber_type']  = trip_data['subscriber_type'].astype('category')\n",
    "trip_data['zip_code']         = trip_data['zip_code'].astype('str')\n",
    "trip_data['duration_minutes'] = trip_data['duration_minutes'].astype('float')\n",
    "\n",
    "\n",
    "print('Trip Data Cleanup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 983648 entries, 913465 to 198776\n",
      "Data columns (total 9 columns):\n",
      "duration            983648 non-null float64\n",
      "start_date          983648 non-null datetime64[ns]\n",
      "start_terminal      983648 non-null category\n",
      "end_date            983648 non-null datetime64[ns]\n",
      "end_terminal        983648 non-null category\n",
      "bike_#              983648 non-null int64\n",
      "subscriber_type     983648 non-null category\n",
      "zip_code            983648 non-null object\n",
      "duration_minutes    983648 non-null float64\n",
      "dtypes: category(3), datetime64[ns](2), float64(2), int64(1), object(1)\n",
      "memory usage: 55.4+ MB\n"
     ]
    }
   ],
   "source": [
    "trip_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>start_date</th>\n",
       "      <th>start_terminal</th>\n",
       "      <th>end_date</th>\n",
       "      <th>end_terminal</th>\n",
       "      <th>bike_#</th>\n",
       "      <th>subscriber_type</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>duration_minutes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>913465</th>\n",
       "      <td>746.0</td>\n",
       "      <td>2015-09-01 00:10:00</td>\n",
       "      <td>69</td>\n",
       "      <td>2015-09-01 00:23:00</td>\n",
       "      <td>58</td>\n",
       "      <td>238</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>94107.0</td>\n",
       "      <td>12.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913466</th>\n",
       "      <td>969.0</td>\n",
       "      <td>2015-09-01 00:15:00</td>\n",
       "      <td>41</td>\n",
       "      <td>2015-09-01 00:31:00</td>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>94133.0</td>\n",
       "      <td>16.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913467</th>\n",
       "      <td>233.0</td>\n",
       "      <td>2015-09-01 00:15:00</td>\n",
       "      <td>42</td>\n",
       "      <td>2015-09-01 00:19:00</td>\n",
       "      <td>45</td>\n",
       "      <td>534</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>94111.0</td>\n",
       "      <td>3.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913468</th>\n",
       "      <td>213.0</td>\n",
       "      <td>2015-09-01 01:29:00</td>\n",
       "      <td>41</td>\n",
       "      <td>2015-09-01 01:32:00</td>\n",
       "      <td>74</td>\n",
       "      <td>312</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>94107.0</td>\n",
       "      <td>3.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913469</th>\n",
       "      <td>574.0</td>\n",
       "      <td>2015-09-01 01:33:00</td>\n",
       "      <td>74</td>\n",
       "      <td>2015-09-01 01:42:00</td>\n",
       "      <td>69</td>\n",
       "      <td>279</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>94107.0</td>\n",
       "      <td>9.566667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         duration          start_date start_terminal            end_date  \\\n",
       "Trip ID                                                                    \n",
       "913465      746.0 2015-09-01 00:10:00             69 2015-09-01 00:23:00   \n",
       "913466      969.0 2015-09-01 00:15:00             41 2015-09-01 00:31:00   \n",
       "913467      233.0 2015-09-01 00:15:00             42 2015-09-01 00:19:00   \n",
       "913468      213.0 2015-09-01 01:29:00             41 2015-09-01 01:32:00   \n",
       "913469      574.0 2015-09-01 01:33:00             74 2015-09-01 01:42:00   \n",
       "\n",
       "        end_terminal  bike_# subscriber_type zip_code  duration_minutes  \n",
       "Trip ID                                                                  \n",
       "913465            58     238      Subscriber  94107.0         12.433333  \n",
       "913466            46      16      Subscriber  94133.0         16.150000  \n",
       "913467            45     534      Subscriber  94111.0          3.883333  \n",
       "913468            74     312      Subscriber  94107.0          3.550000  \n",
       "913469            69     279      Subscriber  94107.0          9.566667  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trip_clean = trip_data.copy()\n",
    "# get list of all time stamps\n",
    "\n",
    "if not pre_cleaned:\n",
    "    trip_start_times = pd.unique(trip_clean.start_date)\n",
    "    trip_start_times = np.sort(trip_start_times)\n",
    "    print('trip start times\\t', len(trip_clean.start_date))\n",
    "    print('unique trip start times\\t', len(trip_start_times))\n",
    "\n",
    "    print()\n",
    "    trip_end_times = pd.unique(trip_clean.end_date)\n",
    "    trip_end_times = np.sort(trip_end_times)\n",
    "    print('trip end times\\t\\t', len(trip_clean.end_date))\n",
    "    print('unique trip end times\\t', len(trip_end_times))\n",
    "    \n",
    "    # create numpy array of only unique timestamps from all trips\n",
    "    unique_times = np.concatenate([trip_start_times, trip_end_times])\n",
    "    unique_times = np.unique(unique_times)\n",
    "    unique_times = np.sort(unique_times)\n",
    "\n",
    "    print()\n",
    "    print('unique trip times\\t', len(unique_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Status Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if not pre_cleaned:\n",
    "    print('Loading Status Data...')\n",
    "\n",
    "    # manual load files\n",
    "    file01 = '../../datasets/bayareabikeshare/201402_status_data.csv'\n",
    "    file02 = '../../datasets/bayareabikeshare/201408_status_data.csv'\n",
    "    file03 = '../../datasets/bayareabikeshare/201508_status_data.csv'\n",
    "    file04 = '../../datasets/bayareabikeshare/201608_status_data.csv'\n",
    "\n",
    "    print('\\nstarted reading ', file01)\n",
    "    status_01 = pd.DateFrame()\n",
    "    status_01 = pd.read_csv(file01, parse_dates=['time'])\n",
    "    print('\\tcleaning time')\n",
    "    # status_01['time']   = pd.to_datetime(status_01['time'],   format=\"%Y/%m/%d %H:%M:%S\")\n",
    "    status_01['time'] = status_01['time'].apply(lambda t: t.replace(second=0))\n",
    "    print('\\tdone!')\n",
    "\n",
    "    print('\\nstarted reading ', file02)\n",
    "    status_02 = pd.DateFrame()\n",
    "    status_02 = pd.read_csv(file02, parse_dates=['time'])\n",
    "    print('\\tcleaning time')\n",
    "    # status_02['time']   = pd.to_datetime(status_02['time'],   format=\"%Y/%m/%d %H:%M:%S\")\n",
    "    status_02['time'] = status_02['time'].apply(lambda t: t.replace(second=0))\n",
    "    print('\\tdone!')\n",
    "\n",
    "    print('\\nstarted reading ', file03)\n",
    "    status_03 = pd.DateFrame()\n",
    "    status_03 = pd.read_csv(file03, parse_dates=['time'])\n",
    "    print('\\tcleaning time')\n",
    "    # status_03['time']   = pd.to_datetime(status_03['time'],   format=\"%Y/%m/%d %H:%M:%S\")\n",
    "    status_03['time'] = status_03['time'].apply(lambda t: t.replace(second=0))\n",
    "    print('\\tdone!')\n",
    "\n",
    "    print('\\nstarted reading ', file04)\n",
    "    status_04 = pd.DateFrame()\n",
    "    status_04 = pd.read_csv(file04)\n",
    "    print('\\tcleaning time')\n",
    "    status_04['time']   = pd.to_datetime(status_04['time'],   format=\"%m/%d/%Y %H:%M:%S\")\n",
    "    print('\\tsetting all time seconds to zero')\n",
    "    status_04['time'] = status_04['time'].apply(lambda t: t.replace(second=0))\n",
    "    print('\\tdone!')\n",
    "else:\n",
    "    print('data was already cleaened and used')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Status data is ultra dense, with records from every station, at ever minutes.\n",
    ">\n",
    "> Previously, we collected unique_trip_times to be an array of all the unique time stamps from all trip start and end dates\n",
    ">\n",
    "> we will now prune the status data down to only include records that are in these unique time stamps.\n",
    ">\n",
    "> this does not completely removed unneessary times, such as a status record for a station that was not used for a given timestamp, but it reduces the number or records to search to less than half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not pre_cleaned:\n",
    "    # status_01 - prune down to only times from unique_trip_times\n",
    "    status_01_pruned = status_01[status_01['time'].isin(unique_times)]\n",
    "    print('pruned status_01 from %s to %s' % (len(status_01), len(status_01_pruned)))\n",
    "\n",
    "    # status_02 - prune down to only times from unique_trip_times\n",
    "    status_02_pruned = status_02[status_02['time'].isin(unique_times)]\n",
    "    print('pruned status_02 from %s to %s' % (len(status_02), len(status_02_pruned)))\n",
    "\n",
    "    # status_03 - prune down to only times from unique_trip_times\n",
    "    status_03_pruned = status_03[status_03['time'].isin(unique_times)]\n",
    "    print('pruned status_03 from %s to %s' % (len(status_03), len(status_03_pruned)))\n",
    "\n",
    "    # status_04 - prune down to only times from unique_trip_times\n",
    "    status_04_pruned = status_04[status_04['time'].isin(unique_times)]\n",
    "    print('pruned status_04 from %s to %s' % (len(status_04), len(status_04_pruned)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not pre_cleaned:\n",
    "    status_selects = pd.concat([status_01_pruned, status_02_pruned, status_03_pruned, status_04_pruned])\n",
    "    status_selects.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not pre_cleaned:\n",
    "    all_records = len(status_01) + len(status_02) + len(status_03) + len(status_04)\n",
    "    print('pruned status_data from %s to %s' % (all_records, len(status_selects)))\n",
    "    print('\\tratio %s' % (len(status_selects) / all_records * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Trip Data with Status Data\n",
    "\n",
    "\n",
    "> for each trip, we want to append a 'bikes_available' column with the number of bikes that were available at that time, at that station\n",
    ">\n",
    "> for each trip, we want to append a 'docks_available' column with the number of docks that were available at that time, at that station\n",
    ">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docks_available_at_end(row):\n",
    "    # lookup bikes that were available at that station\n",
    "    test = status_selects.loc[status_selects['station_id'] == row['end_terminal']]\n",
    "    test = test.loc[test['time'] == row['end_date']]\n",
    "    print('[bikes_available_at_start] - Last updated at: ', datetime.datetime.now())\n",
    "    try:\n",
    "        result = int(test.docks_available)\n",
    "    except:\n",
    "        result = 'NaN'\n",
    "        \n",
    "    return result\n",
    "        \n",
    "    \n",
    "def bikes_available_at_start(row):\n",
    "    # lookup bikes that were available at that station\n",
    "    test = status_selects.loc[status_selects['station_id'] == row['start_terminal']]\n",
    "    test = test.loc[test['time'] == row['start_date']]\n",
    "    print('[bikes_available_at_start] - Last updated at: ', datetime.datetime.now())\n",
    "    try:\n",
    "        result = int(test.bikes_available)\n",
    "    except:\n",
    "        result = 'NaN'\n",
    "        \n",
    "    return result\n",
    "\n",
    "def start_terminal_zip(row):\n",
    "    test = status_selects.loc['status_selects['station_id]' == row[start_terminal']]\n",
    "    return test.zip\n",
    "\n",
    "\n",
    "def end_terminal_zip(row):\n",
    "    test = status_selects.loc['status_selects['station_id]' == row[start_terminal']]\n",
    "    return test.zip\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">For each trip records, append a the following columns\n",
    "- 'docks_available_at_end' : The number of docks available for the rider to choose from when they ended their trip\n",
    "- 'bikes_available_at_start' : The number of bikes available for the rider to choose from when they started their trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not pre_cleaned:\n",
    "    trip_terminal_utilization = trip_clean[['start_date', 'start_terminal', 'end_date', 'end_terminal']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trip_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not pre_cleaned:\n",
    "    print('Total Trips\\t\\t\\t', len(trip_clean), '\\t', '100.0')\n",
    "    # only subsciber trips\n",
    "    subscriber_trips = trip_clean[trip_clean.subscriber_type == 'Subscriber']\n",
    "    print('Subscriber Trips\\t\\t', len(subscriber_trips), '\\t ', (100.* len(subscriber_trips)/len(trip_clean)))\n",
    "\n",
    "    # am_commute_start = datetime.datetime.strptime('07:00', '%H:%M').time()\n",
    "    # print(subscriber_trips.start_date.dt.time > am_commute_start)\n",
    "\n",
    "    # subscriber trips in commute hours\n",
    "    am_commute_start = datetime.datetime.strptime('07:00', '%H:%M').time()\n",
    "    am_commute_end = datetime.datetime.strptime('11:00', '%H:%M').time()\n",
    "    am_subscriber_commute_trips = subscriber_trips[subscriber_trips.start_date.dt.time >= am_commute_start]\n",
    "    am_subscriber_commute_trips = am_subscriber_commute_trips[am_subscriber_commute_trips.start_date.dt.time <= am_commute_end]\n",
    "    print('AM Commute Subscriber Trips\\t', len(am_subscriber_commute_trips), '\\t ', (100.* len(am_subscriber_commute_trips)/len(trip_clean)))\n",
    "\n",
    "    pm_commute_start = datetime.datetime.strptime('16:00', '%H:%M').time()\n",
    "    pm_commute_end = datetime.datetime.strptime('20:00', '%H:%M').time()\n",
    "    pm_subscriber_commute_trips = subscriber_trips[subscriber_trips.start_date.dt.time >= pm_commute_start]\n",
    "    pm_subscriber_commute_trips = pm_subscriber_commute_trips[pm_subscriber_commute_trips.start_date.dt.time <= pm_commute_end]\n",
    "    print('PM Commute Subscriber Trips\\t', len(pm_subscriber_commute_trips), '\\t ', (100.* len(pm_subscriber_commute_trips)/len(trip_clean)))\n",
    "\n",
    "\n",
    "\n",
    "    # from previous analysis, these are the top ten start and end terminals used by subscribers during commute hours\n",
    "    am_start_terms = [50, 54, 55, 61, 67, 69, 70, 73, 74, 77]\n",
    "    am_start_terms = [69, 70, 73, 74, 77]\n",
    "    am_end_terms   = [51, 55, 60, 61, 63, 65, 69, 70, 74, 77]\n",
    "    am_end_terms   = [65, 69, 70, 74, 77]\n",
    "    pm_start_terms = [55, 60, 61, 64, 65, 67, 69, 70, 74, 77]\n",
    "    pm_start_terms = [67, 69, 70, 74, 77]\n",
    "    pm_end_terms   = [39, 50, 55, 60, 61, 65, 69, 70, 74, 77]\n",
    "    pm_end_terms   = [65, 69, 70, 74, 77]\n",
    "\n",
    "\n",
    "    am_sub_start_terms = am_subscriber_commute_trips[am_subscriber_commute_trips.start_terminal.isin(am_start_terms)].copy()\n",
    "    print('am_sub_start_terms: \\t\\t', len(am_sub_start_terms), '\\t ', (100.* len(am_sub_start_terms)/len(trip_clean)))\n",
    "    am_sub_end_terms   = am_subscriber_commute_trips[am_subscriber_commute_trips.end_terminal.isin(am_end_terms)].copy()\n",
    "    print('am_sub_end_terms:\\t\\t ', len(am_sub_end_terms), '\\t ', (100.* len(am_sub_end_terms)/len(trip_clean)))\n",
    "\n",
    "    pm_sub_start_terms = pm_subscriber_commute_trips[pm_subscriber_commute_trips.start_terminal.isin(pm_start_terms)].copy()\n",
    "    print('pm_sub_start_terms: \\t\\t ', len(pm_sub_start_terms), '\\t ', (100.* len(pm_sub_start_terms)/len(trip_clean)))\n",
    "    pm_sub_end_terms   = pm_subscriber_commute_trips[pm_subscriber_commute_trips.end_terminal.isin(pm_end_terms)].copy()\n",
    "    print('pm_sub_end_terms: \\t\\t', len(pm_sub_end_terms), '\\t ', (100.* len(pm_sub_end_terms)/len(trip_clean)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not pre_cleaned:\n",
    "    print('am_sub_start_terms: ', len(am_sub_start_terms))\n",
    "    print('pm_sub_start_terms:  ', len(pm_sub_start_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending Dock Informtion to Trips\n",
    "\n",
    "> We are going to narrow our search to subscribers during morning and evening commute hours\n",
    ">\n",
    "> We will also be looking only at trips that started during these commute hours\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not pre_cleaned:\n",
    "    # pm_sub_start_terms.info()\n",
    "\n",
    "    # print('Appending \\'docks_available_at_end\\'')\n",
    "    # pm_sub_start_terms['docks_available_at_end'] = pm_sub_start_terms.apply(lambda row: docks_available_at_end (row),axis=1)\n",
    "    print('Appending \\'bikes_available_at_start\\'')\n",
    "    pm_sub_start_terms['bikes_available_at_start'] = pm_sub_start_terms.apply(lambda row: bikes_available_at_start(row),axis=1)\n",
    "    print('Done!')\n",
    "\n",
    "    print('Writing to file')\n",
    "    pm_sub_start_terms.to_csv('pm_sub_start_terms.csv', encoding='utf-8')\n",
    "    print('Done!')\n",
    "\n",
    "    pm_sub_start_terms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not pre_cleaned:\n",
    "    # am_sub_start_terms.info()\n",
    "\n",
    "    # print('Appending \\'docks_available_at_end\\'')\n",
    "    am_sub_start_terms['docks_available_at_end'] = am_sub_start_terms.apply(lambda row: docks_available_at_end (row),axis=1)\n",
    "    print('Appending \\'bikes_available_at_start\\'')\n",
    "    am_sub_start_terms['bikes_available_at_start'] = am_sub_start_terms.apply(lambda row: bikes_available_at_start(row),axis=1)\n",
    "    print('Done!')\n",
    "\n",
    "    print('Writing to file')\n",
    "    am_sub_start_terms.to_csv('am_sub_start_terms.csv', encoding='utf-8')\n",
    "    print('Done!')\n",
    "\n",
    "    am_sub_start_terms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trip_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,6))\n",
    "trip_clean.groupby('start_terminal')['bikes_available_at_start'].mean().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='start_terminal', y='bikes_available_at_start', data=trip_clean, x_estimator=np.mean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x='end_terminal', y='docks_available_at_end', data=trip_clean, kind='reg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,6))\n",
    "ax = sns.swarmplot(x=\"start_terminal\", y=\"bikes_available_at_start\", data=trip_clean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,6))\n",
    "ax = sns.swarmplot(x=\"end_terminal\", y=\"docks_available_at_start\", data=trip_clean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bike number vs number of tirps\n",
    "bike_first_trip = trip_clean.groupby('bike_#')['start_date'].min()\n",
    "bike_last_trip = trip_clean.groupby('bike_#')['start_date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bike_first = bike_first_trip.to_frame()\n",
    "bike_last  = bike_last_trip.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "junk = trip_clean.groupby(['start_terminal', 'end_terminal'])['bike_#'].count()\n",
    "junk.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bike_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bike_data = pd.DataFrame()\n",
    "\n",
    "bike_first_trip = trip_clean.groupby('bike_#')['start_date'].min()\n",
    "bike_last_trip = trip_clean.groupby('bike_#')['start_date'].max()\n",
    "\n",
    "\n",
    "\n",
    "bike_first = bike_first_trip.to_frame()\n",
    "bike_last  = bike_last_trip.to_frame()\n",
    "\n",
    "bike_data = pd.concat([bike_first, bike_last], axis=1)\n",
    "\n",
    "bike_data.columns = ['first_trip', 'last_trip']\n",
    "\n",
    "bike_data['days_in_service'] = (bike_data['last_trip'] - bike_data['first_trip']).dt.days\n",
    "\n",
    "bike_data.reset_index(inplace=True)\n",
    "bike_data.columns = ['bike_id', 'first_trip', 'last_trip', 'days_in_service']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bike_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bike_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bike_data['days_in_service'] = (bike_data['last_trip'] - bike_data['first_trip']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bike_data.reset_index(inplace=True)\n",
    "bike_data.columns = ['bike_id', 'first_trip', 'last_trip', 'days_in_service']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bike_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bike_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,6))\n",
    "sns.distplot(bike_data.days_in_service, color='b')\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x=bike_data.bike_id, y=bike_data.days_in_service, data=bike_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bike_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_recorded_trip = bike_data.last_trip.max()\n",
    "bike_data['days_since_last_trip'] = (last_recorded_trip - bike_data.last_trip).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bike_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x='days_in_service', y='days_since_last_trip', data=bike_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.subplots(figsize=(12,6))\n",
    "sns.distplot(bike_data[bike_data.days_since_last_trip > 0]['days_since_last_trip'], color='b')\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
