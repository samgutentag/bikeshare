{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Investigation - Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Station Data...\n",
      "\tFinished file! (1 of 4)\n",
      "\tFinished file! (2 of 4)\n",
      "\tFinished file! (3 of 4)\n",
      "\tFinished file! (4 of 4)\n",
      "Data Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "print('Loading Station Data...')\n",
    "\n",
    "try:\n",
    "    file = 'station_data_clean.csv'\n",
    "    \n",
    "    station_import = pd.read_csv('../junk.csv')\n",
    "except:\n",
    "    try:\n",
    "        file_path_slug = '../../../datasets/bayareabikeshare/*_station_data.csv'\n",
    "        file_list = glob(file_path_slug)\n",
    "\n",
    "        station_import = pd.DataFrame()\n",
    "\n",
    "        counter = 1\n",
    "        chunks = []\n",
    "\n",
    "        for file in file_list:\n",
    "            for chunk in pd.read_csv(file, chunksize=10000, iterator=True):\n",
    "                chunk.columns = ['station_id', 'name', 'lat', 'long', 'dockcount', 'landmark', 'installation']            \n",
    "                chunks.append(chunk)\n",
    "            print('\\tFinished file! (%d of %d)' % (counter, len(file_list)))\n",
    "            counter += 1\n",
    "\n",
    "        station_import = pd.concat(chunks)\n",
    "        print('Data Loaded Successfully!')\n",
    "    except:\n",
    "        print('oops... something went wrong importing the data :(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_zip(row):\n",
    "    if row['landmark'] == 'San Francisco':\n",
    "       return '94107'\n",
    "    if row['landmark'] == 'Redwood City':\n",
    "        return '94063'\n",
    "    if row['landmark'] == 'Palo Alto':\n",
    "        return '94301'\n",
    "    if row['landmark'] == 'Mountain View':\n",
    "        return '94041'\n",
    "    if row['landmark'] == 'San Jose':\n",
    "        return '95113'\n",
    "    return '99999'\n",
    "\n",
    "def make_lat_long(row):\n",
    "    lat = row['lat']\n",
    "    long = row['long']\n",
    "    return (lat, long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station Data Cleanup Started...\n",
      "\tdropping empty rows\n",
      "\tset datatype for each column\n",
      "Cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "print('Station Data Cleanup Started...')\n",
    "\n",
    "station_data = station_import.copy()\n",
    "\n",
    "# remove dulplicates\n",
    "# print('\\tremove dulplicates')\n",
    "# station_data.drop_duplicates(keep='first', inplace=True)\n",
    "print('\\tdropping empty rows')\n",
    "station_data.dropna(how='all', inplace=True)\n",
    "\n",
    "# set datatype for each column\n",
    "print('\\tset datatype for each column')\n",
    "station_data['station_id']   = station_data['station_id'].astype('int')\n",
    "station_data['name']         = station_data['name'].astype('str')\n",
    "station_data['lat']          = station_data['lat'].astype('float')\n",
    "station_data['long']         = station_data['long'].astype('float')\n",
    "station_data['landmark']     = station_data['landmark'].astype('category')\n",
    "\n",
    "# add a zipcode column for later comparison with weather data\n",
    "station_data['zip_code'] = station_data.apply(lambda row: label_zip (row),axis=1)\n",
    "# station_data['zip_code'] = station_data['landmark'].astype('str')\n",
    "\n",
    "# create lat,lon tuple column\n",
    "station_data['lat_long'] = station_data.apply(lambda row: make_lat_long (row),axis=1)\n",
    "\n",
    "station_data.drop_duplicates(station_data.columns, keep='first', inplace=True)\n",
    "\n",
    "station_data.sort_values(by=['station_id', 'installation'], inplace=True)\n",
    "\n",
    "print('Cleaning complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data = station_data.drop_duplicates(subset='station_id', keep='last').set_index('station_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_data.to_csv('../../../datasets/bayareabikeshare/CLEANED/station_data_cleaned.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
