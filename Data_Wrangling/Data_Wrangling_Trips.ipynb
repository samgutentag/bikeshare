{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling- Trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Trip Data...\n",
      "\tFinished file! (1 of 4)\n",
      "\tFinished file! (2 of 4)\n",
      "\tFinished file! (3 of 4)\n",
      "\tFinished file! (4 of 4)\n",
      "Data Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "print('Loading Trip Data...')\n",
    "\n",
    "try:\n",
    "    file_path_slug = '../../../datasets/bayareabikeshare/*_trip_data.csv'\n",
    "\n",
    "    # glob all files\n",
    "    file_list = glob(file_path_slug)\n",
    "\n",
    "    trip_import = pd.DataFrame()\n",
    "\n",
    "    counter = 1\n",
    "    chunks = []\n",
    "\n",
    "    # load data from each file\n",
    "    for file in file_list:\n",
    "\n",
    "        # import file in chunks\n",
    "        for chunk in pd.read_csv(file, chunksize=10000, iterator=True):\n",
    "\n",
    "            # set chunk index column to 'Trip ID'\n",
    "            chunk = chunk.set_index('Trip ID')\n",
    "\n",
    "            # define Columns\n",
    "            chunk.columns = ['Duration', 'Start Date', 'Start Station', 'Start Terminal', 'End Date', \n",
    "                             'End Station', 'End Terminal', 'Bike #', 'Subscriber Type', 'Zip Code']\n",
    "\n",
    "            # append chunk to chunks list\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        print('\\tFinished file! (%d of %d)' % (counter, len(file_list)))\n",
    "        counter += 1\n",
    "\n",
    "    # concat chunks\n",
    "    trip_import = pd.concat(chunks)\n",
    "\n",
    "    print('Data Loaded Successfully!')\n",
    "\n",
    "except:\n",
    "    print('oops... something went wrong importing the data :(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 983648 entries, 913465 to 198776\n",
      "Data columns (total 10 columns):\n",
      "Duration           983648 non-null int64\n",
      "Start Date         983648 non-null object\n",
      "Start Station      983648 non-null object\n",
      "Start Terminal     983648 non-null int64\n",
      "End Date           983648 non-null object\n",
      "End Station        983648 non-null object\n",
      "End Terminal       983648 non-null int64\n",
      "Bike #             983648 non-null int64\n",
      "Subscriber Type    983648 non-null object\n",
      "Zip Code           976838 non-null object\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 82.6+ MB\n"
     ]
    }
   ],
   "source": [
    "trip_import.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zipcodes are all over the place, only keep corrected 5 digit zipcodes, and replace all others with NaNs\n",
    "def clean_zipcode(item):\n",
    "    if len(item) != 5:\n",
    "\n",
    "        # split on '-'\n",
    "        try:\n",
    "            result = item.split('-')[0]\n",
    "        except:\n",
    "            result = item\n",
    "\n",
    "        # split on '.'\n",
    "        try:\n",
    "            result = item.split('.')[0]\n",
    "        except:\n",
    "            result = item\n",
    "        \n",
    "        # if len of item is less than 5, return 'NaN'\n",
    "        if len(result) < 5:\n",
    "            result = 'NaN'\n",
    "        else:\n",
    "            # if len result is greater than 5, take at most, first 5 digits\n",
    "            result = result[:5]\n",
    "    else:\n",
    "        result = item\n",
    "    \n",
    "    # make sure result is all digits\n",
    "    if result.isdigit():\n",
    "        return result\n",
    "    else:\n",
    "        return 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip Data Cleanup Started...\n",
      "\tcleaning column names\n",
      "\tsubsetting to useful columns\n",
      "\tcreating a duration_minutes column\n",
      "\tconverting end and start dates to datetime objects\n",
      "\tcleaning zipcodes\n",
      "cleaning up data types\n",
      "Trip Data Cleanup complete\n"
     ]
    }
   ],
   "source": [
    "print('Trip Data Cleanup Started...')\n",
    "trip_data = trip_import.copy()\n",
    "\n",
    "# cleanup column names\n",
    "print('\\tcleaning column names')\n",
    "new_cols = []\n",
    "for col in trip_data.columns:\n",
    "    new_cols.append(col.replace(' ', '_').lower())\n",
    "trip_data.columns = new_cols\n",
    "\n",
    "# extract columns we want to keep\n",
    "print('\\tsubsetting to useful columns')\n",
    "important_cols = ['duration', 'start_date', 'start_terminal', 'start_station', 'end_date', 'end_terminal', 'end_station', 'bike_#', 'subscriber_type', 'zip_code']\n",
    "trip_data = trip_data[important_cols]\n",
    "\n",
    "# we are only looking at stations in San Francisco\n",
    "# sf_trips_data = trip_data[trip_data['start_terminal'].isin(sf_stations)]\n",
    "# sf_trips_data = sf_trips_data[sf_trips_data['end_terminal'].isin(sf_stations)]\n",
    "\n",
    "# trip_data = sf_trips_data.copy()\n",
    "\n",
    "# create duration minutes column\n",
    "print('\\tcreating a duration_minutes column')\n",
    "trip_data['duration_minutes'] = trip_data['duration'] / 60.0\n",
    "\n",
    "# convert end and start dates to datetime objects\n",
    "print('\\tconverting end and start dates to datetime objects')\n",
    "trip_data['start_date'] = pd.to_datetime(trip_data['start_date'], format=\"%m/%d/%Y %H:%M\")\n",
    "trip_data['end_date']   = pd.to_datetime(trip_data['end_date'],   format=\"%m/%d/%Y %H:%M\")\n",
    "\n",
    "\n",
    "# convert and clean zipcodes\n",
    "print('\\tcleaning zipcodes')\n",
    "trip_data['zip_code'] = trip_data['zip_code'].astype(str)\n",
    "trip_data.zip_code = trip_data.zip_code.apply(clean_zipcode)\n",
    "trip_data['zip_code'] = pd.to_numeric(trip_data['zip_code'], errors='coerce')\n",
    "\n",
    "# clean up data types\n",
    "print('cleaning up data types')\n",
    "\n",
    "trip_data['duration']         = trip_data['duration'].astype('float')\n",
    "trip_data['start_terminal']   = trip_data['start_terminal'].astype('category')\n",
    "trip_data['end_terminal']     = trip_data['end_terminal'].astype('category')\n",
    "trip_data['bike_#']           = trip_data['bike_#'].astype('int')\n",
    "trip_data['subscriber_type']  = trip_data['subscriber_type'].astype('category')\n",
    "trip_data['zip_code']         = trip_data['zip_code'].astype('str')\n",
    "trip_data['duration_minutes'] = trip_data['duration_minutes'].astype('float')\n",
    "\n",
    "trip_data.rename(columns={'bike_#': 'bike_id'}, inplace=True)\n",
    "trip_data.rename(columns={'zip_code': 'user_zip'}, inplace=True)\n",
    "trip_data.rename(columns={'subscriber_type': 'user_type'}, inplace=True)\n",
    "\n",
    "print('Trip Data Cleanup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trip_data_sorted = trip_data.sort_values('start_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 983648 entries, 4069 to 1338408\n",
      "Data columns (total 11 columns):\n",
      "duration            983648 non-null float64\n",
      "start_date          983648 non-null datetime64[ns]\n",
      "start_terminal      983648 non-null category\n",
      "start_station       983648 non-null object\n",
      "end_date            983648 non-null datetime64[ns]\n",
      "end_terminal        983648 non-null category\n",
      "end_station         983648 non-null object\n",
      "bike_id             983648 non-null int64\n",
      "user_type           983648 non-null category\n",
      "user_zip            983648 non-null object\n",
      "duration_minutes    983648 non-null float64\n",
      "dtypes: category(3), datetime64[ns](2), float64(2), int64(1), object(3)\n",
      "memory usage: 70.4+ MB\n"
     ]
    }
   ],
   "source": [
    "trip_data_sorted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>start_date</th>\n",
       "      <th>start_terminal</th>\n",
       "      <th>start_station</th>\n",
       "      <th>end_date</th>\n",
       "      <th>end_terminal</th>\n",
       "      <th>end_station</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>user_zip</th>\n",
       "      <th>duration_minutes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4069</th>\n",
       "      <td>174.0</td>\n",
       "      <td>2013-08-29 09:08:00</td>\n",
       "      <td>64</td>\n",
       "      <td>2nd at South Park</td>\n",
       "      <td>2013-08-29 09:11:00</td>\n",
       "      <td>64</td>\n",
       "      <td>2nd at South Park</td>\n",
       "      <td>288</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>94114.0</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         duration          start_date start_terminal      start_station  \\\n",
       "Trip ID                                                                   \n",
       "4069        174.0 2013-08-29 09:08:00             64  2nd at South Park   \n",
       "\n",
       "                   end_date end_terminal        end_station  bike_id  \\\n",
       "Trip ID                                                                \n",
       "4069    2013-08-29 09:11:00           64  2nd at South Park      288   \n",
       "\n",
       "          user_type user_zip  duration_minutes  \n",
       "Trip ID                                         \n",
       "4069     Subscriber  94114.0               2.9  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_data_sorted.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>start_date</th>\n",
       "      <th>start_terminal</th>\n",
       "      <th>start_station</th>\n",
       "      <th>end_date</th>\n",
       "      <th>end_terminal</th>\n",
       "      <th>end_station</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>user_zip</th>\n",
       "      <th>duration_minutes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1338408</th>\n",
       "      <td>378.0</td>\n",
       "      <td>2016-08-31 23:32:00</td>\n",
       "      <td>46</td>\n",
       "      <td>Washington at Kearny</td>\n",
       "      <td>2016-08-31 23:38:00</td>\n",
       "      <td>60</td>\n",
       "      <td>Embarcadero at Sansome</td>\n",
       "      <td>667</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>94111.0</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         duration          start_date start_terminal         start_station  \\\n",
       "Trip ID                                                                      \n",
       "1338408     378.0 2016-08-31 23:32:00             46  Washington at Kearny   \n",
       "\n",
       "                   end_date end_terminal             end_station  bike_id  \\\n",
       "Trip ID                                                                     \n",
       "1338408 2016-08-31 23:38:00           60  Embarcadero at Sansome      667   \n",
       "\n",
       "          user_type user_zip  duration_minutes  \n",
       "Trip ID                                         \n",
       "1338408  Subscriber  94111.0               6.3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_data_sorted.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune Trip Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune data to exclude trips longer than 60 minutes\n",
    "print('pruning data to trips no more than 60 minutes long...')\n",
    "trip_data = trip_data_sorted[trip_data_sorted['duration_minutes'] <= 60].copy()\n",
    "\n",
    "# Cleanup\n",
    "trip_data.sort_index(inplace=True)\n",
    "print('\\tpruned data set \\'trips\\' consists of %i entries' % len(trip_data.index))\n",
    "\n",
    "plt.subplots(figsize=(12,6))\n",
    "ax = sns.distplot(trip_data[trip_data.user_type == 'Subscriber'].start_date.dt.hour, color='b', label='Subscribers')\n",
    "sns.distplot(trip_data[trip_data.user_type == 'Customer'].start_date.dt.hour, color='r', label='Customers', ax=ax)\n",
    "ax.set(xlabel='start hour')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "955557/983648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trip_data.to_csv('../../../datasets/bayareabikeshare/CLEANED/trip_data_cleaned.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split out only Subscriber Trips\n",
    "trip_data_subscribers = trip_data[trip_data.user_type == 'Subscriber']\n",
    "\n",
    "# Split out only Customer Trips\n",
    "trip_data_customers = trip_data[trip_data.user_type == 'Customer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trip_data_subscribers.to_csv('../../../datasets/bayareabikeshare/CLEANED/trip_data_cleaned_subscribers.csv', encoding='utf-8')\n",
    "trip_data_customers.to_csv('../../../datasets/bayareabikeshare/CLEANED/trip_data_cleaned_customers.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_sub_trips = trip_data_subscribers.groupby(trip_data_subscribers['start_date'].dt.hour)['bike_id'].sum().to_frame()\n",
    "hourly_sub_trips.plot(kind='bar', color='b', figsize=(15,6))\n",
    "plt.show()\n",
    "\n",
    "hourly_cust_trips = trip_data_customers.groupby(trip_data_customers['start_date'].dt.hour)['bike_id'].sum().to_frame()\n",
    "hourly_cust_trips.plot(kind='bar', color='r', figsize=(15,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up Subscriber Morning and Evening Commute Time Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_morning_commuters = trip_data_subscribers[trip_data_subscribers.start_date.dt.hour >= 7].copy()\n",
    "sub_morning_commuters = sub_morning_commuters[sub_morning_commuters.end_date.dt.hour <= 11]\n",
    "print(len(sub_morning_commuters))\n",
    "\n",
    "sub_evening_commuters = trip_data_subscribers[trip_data_subscribers.start_date.dt.hour >= 16].copy()\n",
    "sub_evening_commuters = sub_evening_commuters[sub_evening_commuters.end_date.dt.hour <= 20]\n",
    "print(len(sub_evening_commuters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add some additional information to Commuter trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append start and end terminal zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cleaned station data\n",
    "\n",
    "station_data = pd.DataFrame()\n",
    "station_data = pd.read_csv('../../../datasets/bayareabikeshare/CLEANED/station_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morning Commute Station Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_morning_commuters = trip_data_subscribers[trip_data_subscribers.start_date.dt.hour >= 7].copy()\n",
    "sub_morning_commuters = sub_morning_commuters[sub_morning_commuters.end_date.dt.hour <= 11]\n",
    "sub_morning_commuters.reset_index(inplace=True)\n",
    "\n",
    "sub_morning_commuters.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_subset = station_data[['station_id', 'name', 'lat', 'long', 'dockcount', 'landmark', 'zip_code', 'lat_long']]\n",
    "station_subset.head(40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add start_terminal info\n",
    "sub_morning_commuters = pd.merge(sub_morning_commuters, \n",
    "                  station_subset, \n",
    "                  how='left', \n",
    "                  left_on=['start_terminal', 'start_station'], \n",
    "                  right_on=['station_id', 'name'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nans = lambda df: df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans(sub_morning_commuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop station_id and name\n",
    "sub_morning_commuters.drop(['station_id', 'name'], axis=1, inplace=True)\n",
    "\n",
    "# rename columns for start terminal\n",
    "sub_morning_commuters.rename(columns={'lat' : 'start_terminal_lat', 'long' : 'start_terminal_long', 'dockcount' : 'start_terminal_dockcount', 'landmark' : 'start_terminal_landmark', 'zip_code' : 'start_terminal_zip_code', 'lat_long' : 'start_terminal_lat_long'}, inplace=True)\n",
    "\n",
    "# add end_terminal info\n",
    "sub_morning_commuters = pd.merge(sub_morning_commuters, \n",
    "                  station_subset, \n",
    "                  how='left', \n",
    "                  left_on=['end_terminal', 'end_station'], \n",
    "                  right_on=['station_id', 'name'])\n",
    "# drop station_id and name\n",
    "sub_morning_commuters.drop(['station_id', 'name'], axis=1, inplace=True)\n",
    "\n",
    "# rename columns for start terminal\n",
    "sub_morning_commuters.rename(columns={'lat' : 'end_terminal_lat', 'long' : 'end_terminal_long', 'dockcount' : 'end_terminal_dockcount', 'landmark' : 'end_terminal_landmark', 'zip_code' : 'end_terminal_zip_code', 'lat_long' : 'end_terminal_lat_long'}, inplace=True)\n",
    "\n",
    "# set index\n",
    "sub_morning_commuters.set_index('Trip ID', inplace=True)\n",
    "sub_morning_commuters.info()\n",
    "\n",
    "sub_morning_commuters.to_csv('../../../datasets/bayareabikeshare/CLEANED/trip_data_cleaned_morning_commuters.csv', encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_daylight_trips = trip_data_customers[trip_data_customers.start_date.dt.hour >= 8].copy()\n",
    "customer_daylight_trips = customer_daylight_trips[customer_daylight_trips.end_date.dt.hour <= 19]\n",
    "customer_daylight_trips.reset_index(inplace=True)\n",
    "\n",
    "customer_daylight_trips.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_morning_commuters = trip_data_subscribers[trip_data_subscribers.start_date.dt.hour >= 7].copy()\n",
    "sub_morning_commuters = sub_morning_commuters[sub_morning_commuters.end_date.dt.hour <= 11]\n",
    "sub_morning_commuters.reset_index(inplace=True)\n",
    "\n",
    "station_subset = station_data[['station_id', 'name', 'lat', 'long', 'dockcount', 'landmark', 'zip_code', 'lat_long']]\n",
    "\n",
    "# add start_terminal info\n",
    "sub_morning_commuters = pd.merge(sub_morning_commuters, \n",
    "                  station_subset, \n",
    "                  how='left', \n",
    "                  left_on=['start_terminal', 'start_station'], \n",
    "                  right_on=['station_id', 'name'])\n",
    "\n",
    "# drop station_id and name\n",
    "sub_morning_commuters.drop(['station_id', 'name'], axis=1, inplace=True)\n",
    "\n",
    "# rename columns for start terminal\n",
    "sub_morning_commuters.rename(columns={'lat' : 'start_terminal_lat', 'long' : 'start_terminal_long', 'dockcount' : 'start_terminal_dockcount', 'landmark' : 'start_terminal_landmark', 'zip_code' : 'start_terminal_zip_code', 'lat_long' : 'start_terminal_lat_long'}, inplace=True)\n",
    "\n",
    "# add end_terminal info\n",
    "sub_morning_commuters = pd.merge(sub_morning_commuters, \n",
    "                  station_subset, \n",
    "                  how='left', \n",
    "                  left_on=['end_terminal', 'end_station'], \n",
    "                  right_on=['station_id', 'name'])\n",
    "# drop station_id and name\n",
    "sub_morning_commuters.drop(['station_id', 'name'], axis=1, inplace=True)\n",
    "\n",
    "# rename columns for start terminal\n",
    "sub_morning_commuters.rename(columns={'lat' : 'end_terminal_lat', 'long' : 'end_terminal_long', 'dockcount' : 'end_terminal_dockcount', 'landmark' : 'end_terminal_landmark', 'zip_code' : 'end_terminal_zip_code', 'lat_long' : 'end_terminal_lat_long'}, inplace=True)\n",
    "\n",
    "# set index\n",
    "sub_morning_commuters.set_index('Trip ID', inplace=True)\n",
    "sub_morning_commuters.info()\n",
    "\n",
    "sub_morning_commuters.to_csv('../../../datasets/bayareabikeshare/CLEANED/trip_data_cleaned_morning_commuters.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evening Commute Station Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_evening_commuters = trip_data_subscribers[trip_data_subscribers.start_date.dt.hour >= 16].copy()\n",
    "sub_evening_commuters = sub_evening_commuters[sub_evening_commuters.end_date.dt.hour <= 20]\n",
    "sub_evening_commuters.reset_index(inplace=True)\n",
    "station_subset = station_data[['station_id', 'name', 'lat', 'long', 'dockcount', 'landmark', 'zip_code', 'lat_long']]\n",
    "\n",
    "# add start_terminal info\n",
    "sub_evening_commuters = pd.merge(sub_evening_commuters, \n",
    "                  station_subset, \n",
    "                  how='left', \n",
    "                  left_on=['start_terminal', 'start_station'], \n",
    "                  right_on=['station_id', 'name'])\n",
    "\n",
    "# drop station_id and name\n",
    "sub_evening_commuters.drop(['station_id', 'name'], axis=1, inplace=True)\n",
    "\n",
    "# rename columns for start terminal\n",
    "sub_evening_commuters.rename(columns={'lat' : 'start_terminal_lat', \n",
    "                                      'long' : 'start_terminal_long', \n",
    "                                      'dockcount' : 'start_terminal_dockcount', \n",
    "                                      'landmark' : 'start_terminal_landmark', \n",
    "                                      'zip_code' : 'start_terminal_zip_code', \n",
    "                                      'lat_long' : 'start_terminal_lat_long'}, inplace=True)\n",
    "\n",
    "# add end_terminal info\n",
    "sub_evening_commuters = pd.merge(sub_evening_commuters, \n",
    "                  station_subset, \n",
    "                  how='left', \n",
    "                  left_on=['end_terminal', 'end_station'], \n",
    "                  right_on=['station_id', 'name'])\n",
    "# drop station_id and name\n",
    "sub_evening_commuters.drop(['station_id', 'name'], axis=1, inplace=True)\n",
    "\n",
    "# rename columns for start terminal\n",
    "sub_evening_commuters.rename(columns={'lat' : 'end_terminal_lat', \n",
    "                                      'long' : 'end_terminal_long', \n",
    "                                      'dockcount' : 'end_terminal_dockcount', \n",
    "                                      'landmark' : 'end_terminal_landmark', \n",
    "                                      'zip_code' : 'end_terminal_zip_code', \n",
    "                                      'lat_long' : 'end_terminal_lat_long'}, inplace=True)\n",
    "\n",
    "# set index\n",
    "sub_evening_commuters.set_index('Trip ID', inplace=True)\n",
    "sub_evening_commuters.info()\n",
    "\n",
    "sub_evening_commuters.to_csv('../../../datasets/bayareabikeshare/CLEANED/trip_data_cleaned_evening_commuters.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned Commuter Trip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Cleaned Files\n",
    "\n",
    "morning_commuter_trips = pd.read_csv('../../../datasets/bayareabikeshare/CLEANED/trip_data_cleaned_morning_commuters.csv', parse_dates=['start_date', 'end_date'])\n",
    "evening_commuter_trips = pd.read_csv('../../../datasets/bayareabikeshare/CLEANED/trip_data_cleaned_evening_commuters.csv', parse_dates=['start_date', 'end_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morning_commuter_trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "junk = morning_commuter_trips[morning_commuter_trips['start_terminal_zip_code'].isnull()]\n",
    "junk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Append Weather Data to trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cleaned weather data\n",
    "weather_data = pd.DataFrame()\n",
    "weather_data = pd.read_csv('../../../datasets/bayareabikeshare/CLEANED/weather_cleaned_all.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evening_commuter_trips_weather = pd.merge(evening_commuter_trips, \n",
    "                                          weather_data, \n",
    "                                          how='left', \n",
    "                                          left_on=[evening_commuter_trips['start_date'].dt.date, 'start_terminal_zip_code'], \n",
    "                                          right_on=[weather_data['date'].dt.date, 'zip'])\n",
    "evening_commuter_trips_weather.drop(['date', 'zip'], axis=1, inplace=True)\n",
    "evening_commuter_trips_weather.set_index('Trip ID', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "morning_commuter_trips_weather = pd.merge(morning_commuter_trips, \n",
    "                                          weather_data, \n",
    "                                          how='left', \n",
    "                                          left_on=[morning_commuter_trips['start_date'].dt.date, 'start_terminal_zip_code'], \n",
    "                                          right_on=[weather_data['date'].dt.date, 'zip'])\n",
    "morning_commuter_trips_weather.drop(['date', 'zip'], axis=1, inplace=True)\n",
    "morning_commuter_trips_weather.set_index('Trip ID', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evening_commuter_trips_weather.to_csv('../../../datasets/bayareabikeshare/CLEANED/trip_data_cleaned_evening_commuters_weather.csv', encoding='utf-8')\n",
    "morning_commuter_trips_weather.to_csv('../../../datasets/bayareabikeshare/CLEANED/trip_data_cleaned_morning_commuters_weather.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evening_commuter_trips_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Append weather and station data to all trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips = trip_data.copy()\n",
    "\n",
    "print(len(trips))\n",
    "\n",
    "\n",
    "trips.reset_index(inplace=True)\n",
    "station_subset = station_data[['station_id', 'name', 'lat', 'long', 'dockcount', 'landmark', 'zip_code', 'lat_long']]\n",
    "\n",
    "\n",
    "print('appending start terminal info')\n",
    "# add start_terminal info\n",
    "trips = pd.merge(trips, \n",
    "                  station_subset, \n",
    "                  how='left', \n",
    "                  left_on=['start_terminal', 'start_station'], \n",
    "                  right_on=['station_id', 'name'])\n",
    "\n",
    "\n",
    "\n",
    "# drop station_id and name\n",
    "trips.drop(['station_id', 'name'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# rename columns for start terminal\n",
    "trips.rename(columns={'lat' : 'start_terminal_lat', 'long' : 'start_terminal_long', 'dockcount' : 'start_terminal_dockcount', 'landmark' : 'start_terminal_landmark', 'zip_code' : 'start_terminal_zip_code', 'lat_long' : 'start_terminal_lat_long'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('appending end terminal info')\n",
    "# add end_terminal info\n",
    "trips = pd.merge(trips, \n",
    "                  station_subset, \n",
    "                  how='left', \n",
    "                  left_on=['end_terminal', 'end_station'], \n",
    "                  right_on=['station_id', 'name'])\n",
    "# drop station_id and name\n",
    "trips.drop(['station_id', 'name'], axis=1, inplace=True)\n",
    "\n",
    "# rename columns for start terminal\n",
    "trips.rename(columns={'lat' : 'end_terminal_lat', 'long' : 'end_terminal_long', 'dockcount' : 'end_terminal_dockcount', 'landmark' : 'end_terminal_landmark', 'zip_code' : 'end_terminal_zip_code', 'lat_long' : 'end_terminal_lat_long'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "print('appending weather info')\n",
    "trips = pd.merge(trips, \n",
    "                          weather_data, \n",
    "                          how='left', \n",
    "                          left_on=[trips['start_date'].dt.date, 'start_terminal_zip_code'], \n",
    "                          right_on=[weather_data['date'].dt.date, 'zip'])\n",
    "trips.drop(['date', 'zip'], axis=1, inplace=True)\n",
    "trips.set_index('Trip ID', inplace=True)\n",
    "\n",
    "\n",
    "trips.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips.to_csv('../../../datasets/bayareabikeshare/CLEANED/trip_data_cleaned_master.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
